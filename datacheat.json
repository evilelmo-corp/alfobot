[{"Data Science":"Es un campo interdisciplinar, podemos ver en el gráfico, mezcla programación , estadística y matemáticas y conocimiento específico",
"Ciclo":"1.Recoger y Almacenar información \n 2.Analizar y crear \n 3. Poner en producción",
"Machine learning":"Se divide en:\n Supervisado y no supervisado.\n Algoritmo al que no le decimos a un ordenador que hacer respecto a un escenario dado, sino que lo exponemos a diferentes escenarios donde el ordenador define sus parámetros y puede responder a nuevos escenarios.",
"Modelos Supervisados":"Existen dos tipos:\n Clasificación o de regresión",
"Modelos No supervisados":"Existen dos tipos:\n Clustering o de Reducción de dimensionalidad",
"Modelos de clasificación":"K-nearest neighbors (kNN)\n Regresión logística \n Máquinas de soporte vectorial (SVM) \n Naïve bayes (NB) \n Árboles de decisión",
"Modelos de regresión":"Lineal \n lineal simple \n polinomial",

"Regresión lineal":"Técnica de modelado estadístico con la que se describe una variable continua como una función de una o varias variables predictoras.",
"Regresión lineal simple":"Técnica de modelado estadístico con la que se describe una variable continua como una función de una variable predictora.",
"Regresión polinomial":"Forma de regresión lineal en la que la relación entre la variable dependiente e independiente se modela como un polinomio de rgado n",
"Clasificador Euclidiano":"Modelo de clasificación que utiliza la distancia euclidiana.",
"Clasificador kNN":"'k Nearest Neighbors' \n Modelo de clasificación",

"help":"Dime cosas de data science y te doy su descripción. \n\n Para ver un esquema global del Machine learning escribe 'ml sch'. Te saldrá un listado de todo lo que puedo decirte y entre paréntesis una abreviatura ante la cual, si la pones seguida de 'cod' te daré su código. \n \n Por ejemplo, si quieres saber el código de Naive-Bayes, pon: \n ml sch \n ves que su código es nb y acto seguido pones \n cod nb.",
"ml sch":"Regresión: \n -Regresión lineal (lr) \n -Regresión multilineal (mr) \n -Regresión polinomial (pr) \n\n Clasificación: \n -Euclidiano \n -kNN (knn) \n -Naive-Bayes (nb) \n -Regresión logística (logr) \n -Árbol de decisión (dt) \n --Random Forests (rf) \n\n Clustering: \n -k-means (km) \n -Hierarquical clustering (hc) \n\n Útiles: \n -Escalar (sc) \n -train_test_split (tts)",


"cod sc":"from sklearn.preprocessing import MinMaxScaler \n scaler_x = MinMaxScaler() \n scaler_x.fit(X) \n X = scaler_x.transform(X)",
"cod tts":"from sklearn.model_selection import train_test_split \n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)",

"cod lr":"from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=3500) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n\n clf.coef_ \n clf.intercept_",
"cod mr":"from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=3500) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n\n clf.coef_ \n clf.intercept_",
"cod pr":"from sklearn import linear_model \n from sklearn.preprocessing import PolynomialFeatures \n poly = PolynomialFeatures(degree=2) \n X_train_poly = poly.fit_transform(X_train) \n X_train_poly \n\n regresion_poly = linear_model.LinearRegression() \n regresion_poly.fit(X_train_poly, y_train) \n\n clf.coef_ \n clf.intercept_ \n\n yhat = regresion_poly.predict(poly.transform(X_test))",


"cod knn":"from sklearn.neighbors import KNeighborsClassifier \n clasificador = KNeighborsClassifier(100, algorithm='brute') \n clasificador.fit(X_train, y_train) \n yhat = clasificador.predict(X_test)",
"cod nb":"from sklearn.naive_bayes import GaussianNB \n gnb = GaussianNB() \n gnb.fit(X_train, y_train) \n y_pred = gnb.predict(X_test)\n print('Accuracy: ', accuracy_score(y_test,y_pred))",
"cod logr":"from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=150) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n accuracy_score(y_test, yhat) \n\n clf.coef_ \n clf.intercept_",
"cod dt":"from sklearn import tree \n data = df[['...','...',...]] \n target = df['...'] \n clf = tree.DecisionTreeClassifier(criterion='entropy') \n clf = clf.fit(data, target)",
"cod rf":"from sklearn.ensemble import RandomForestClassifier \n clf = RandomForestClassifier() \n clf = clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n print('Accuracy score: ', accuracy_score(y_test,yhat)) \n print('F1-score: ', f1_score(y_test,yhat, average='macro')) \n print('Mean squared error: ', mean_squared_error(y_test,yhat) ) \n confusion_matrix(y_test, yhat)",

"cod km":"from sklearn.cluster import KMeans \n kmeans = KMeans(n_clusters=2) \n kmeans.fit(df_escalado) \n kmeans.labels_ \n #Centroides \n kmeans.cluster_centers_",
"cod hc":"from sklearn.cluster import AgglomerativeClustering \n #Dendograma para visualizar y elegir el corte \n from scipy.spatial import distance_matrix \n dist_matrix = distance_matrix(X,X) \n print(dist_matrix, dist_matrix.shape) \n from scipy.cluster import hierarchy \n from scipy.cluster.hierarchy import ClusterWarning \n from warnings import simplefilter \n simplefilter('ignore', ClusterWarning) \n Z = hierarchy.linkage(dist_matrix, 'complete') \n\n plt.figure(figsize=(20,10)) \n dendro = hierarchy.dendrogram(Z) \n plt.tick_params(axis='x', labelsize=8) \n agglom = AgglomerativeClustering(n_clusters = 3, linkage = 'average') \n agglom.fit(X) \n agglom.n_clusters_ \n agglom.labels_ \n\n plt.figure(figsize=(8,6)) \n sns.scatterplot(df['a1'],df['a2'],hue = df['clase'], palette='Set2') \n plt.show() \n\n agglom.n_leaves_"






}]