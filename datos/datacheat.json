[{"Data Science":"Es un campo interdisciplinar, podemos ver en el gráfico, mezcla programación , estadística y matemáticas y conocimiento específico",
"Ciclo de la ciencia de datos":"1.Recoger y Almacenar información \n 2.Analizar y crear \n 3. Poner en producción",
"Machine learning":"Se divide en:\n Supervisado y no supervisado.\n Algoritmo al que no le decimos a un ordenador que hacer respecto a un escenario dado, sino que lo exponemos a diferentes escenarios donde el ordenador define sus parámetros y puede responder a nuevos escenarios.",
"Modelos Supervisados":"Modelos que contrastan resultados existentes con las predicciones.\n\n Existen dos tipos:\n Clasificación o de regresión",
"Modelos No supervisados":"Modelos que no constrastan los resultados, con resultados reales. \n\n Existen dos tipos:\n Clustering o de Reducción de dimensionalidad",
"Modelos de clasificación":"K-nearest neighbors (kNN)\n Regresión logística \n Máquinas de soporte vectorial (SVM) \n Naïve bayes (NB) \n Árboles de decisión",
"Modelos de regresión":"Lineal \n lineal simple \n polinomial",

"Regresión lineal":"Técnica de modelado estadístico con la que se describe una variable continua como una función de una o varias variables predictoras.",
"Regresión lineal simple":"Técnica de modelado estadístico con la que se describe una variable continua como una función de una variable predictora.",
"Regresión polinomial":"Forma de regresión lineal en la que la relación entre la variable dependiente e independiente se modela como un polinomio de rgado n",
"Clasificador Euclidiano":"Modelo de clasificación que utiliza la distancia euclidiana.",
"Clasificador kNN":"'k Nearest Neighbors' \n Modelo de clasificación",

"help":"Dime cosas de data science y te doy su descripción. \n\n Para ver un esquema global del Machine learning escribe 'ml sch'. Te saldrá un listado de todo lo que puedo decirte y entre paréntesis una abreviatura ante la cual, si la pones seguida de 'cod' te daré su código. \n \n Por ejemplo, si quieres saber el código de Naive-Bayes, pon: \n ml sch \n ves que su código es nb y acto seguido pones \n cod nb.",
"ml sch":"**Regresión:** \n -Regresión lineal (lr) \n -Regresión multilineal (mr) \n -Regresión polinomial (pr) \n\n **Clasificación:** \n -Euclidiano \n -kNN (knn) \n -Naive-Bayes (nb) \n -Regresión logística (logr) \n -Árbol de decisión (dt) \n --Random Forests (rf) \n\n **Clustering:** \n -k-means (km) \n -Hierarquical clustering (hc) \n\n **Reducción de dimensionalidad:**\n PCA: (pca) \n PCA varianza con gráfico: (vpca) \n\n**Útiles:** \n -Escalar (sc) \n -train_test_split (tts)\n  -accuracy scores (acs) \n\n **Validación:** \n -Hold out (ho)\n leave one out (loo) \n kfold cross (kfc) \n stratified kfold (skf)" ,


"cod sc":"```from sklearn.preprocessing import MinMaxScaler \n scaler_x = MinMaxScaler() \n scaler_x.fit(X) \n X = scaler_x.transform(X)```",
"cod tts":"```from sklearn.model_selection import train_test_split \n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)#stratify=y```",

"cod lr":"```from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=3500) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n\n clf.coef_ \n clf.intercept_```",
"cod mr":"```from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=3500) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n\n clf.coef_ \n clf.intercept_```",
"cod pr":"```from sklearn import linear_model \n from sklearn.preprocessing import PolynomialFeatures \n poly = PolynomialFeatures(degree=2) \n X_train_poly = poly.fit_transform(X_train) \n X_train_poly \n\n regresion_poly = linear_model.LinearRegression() \n regresion_poly.fit(X_train_poly, y_train) \n\n clf.coef_ \n clf.intercept_ \n\n yhat = regresion_poly.predict(poly.transform(X_test))```",


"cod knn":"```from sklearn.neighbors import KNeighborsClassifier \n clasificador = KNeighborsClassifier(100, algorithm='brute') \n clasificador.fit(X_train, y_train) \n yhat = clasificador.predict(X_test)```",
"cod nb":"```from sklearn.naive_bayes import GaussianNB \n gnb = GaussianNB() \n gnb.fit(X_train, y_train) \n y_pred = gnb.predict(X_test)\n print('Accuracy: ', accuracy_score(y_test,y_pred))```",
"cod logr":"from sklearn.linear_model import LogisticRegression \n clf = LogisticRegression(max_iter=150) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n accuracy_score(y_test, yhat) \n\n clf.coef_ \n clf.intercept_",
"cod dt":"```from sklearn import tree \n data = df[['...','...',...]] \n target = df['...'] \n clf = tree.DecisionTreeClassifier(criterion='entropy') \n clf = clf.fit(data, target)```",
"cod rf":"```from sklearn.ensemble import RandomForestClassifier \n clf = RandomForestClassifier() \n clf = clf.fit(X_train, y_train) \n yhat = clf.predict(X_test) \n print('Accuracy score: ', accuracy_score(y_test,yhat)) \n print('F1-score: ', f1_score(y_test,yhat, average='macro')) \n print('Mean squared error: ', mean_squared_error(y_test,yhat) ) \n confusion_matrix(y_test, yhat)```",

"cod km":"```from sklearn.cluster import KMeans \n kmeans = KMeans(n_clusters=2) \n kmeans.fit(df_escalado) \n kmeans.labels_ \n #Centroides \n kmeans.cluster_centers_```",
"cod hc":"```from sklearn.cluster import AgglomerativeClustering \n #Dendograma para visualizar y elegir el corte \n from scipy.spatial import distance_matrix \n dist_matrix = distance_matrix(X,X) \n print(dist_matrix, dist_matrix.shape) \n from scipy.cluster import hierarchy \n from scipy.cluster.hierarchy import ClusterWarning \n from warnings import simplefilter \n simplefilter('ignore', ClusterWarning) \n Z = hierarchy.linkage(dist_matrix, 'complete') \n\n plt.figure(figsize=(20,10)) \n dendro = hierarchy.dendrogram(Z) \n plt.tick_params(axis='x', labelsize=8) \n agglom = AgglomerativeClustering(n_clusters = 3, linkage = 'average') \n agglom.fit(X) \n agglom.n_clusters_ \n agglom.labels_ \n\n plt.figure(figsize=(8,6)) \n sns.scatterplot(df['a1'],df['a2'],hue = df['clase'], palette='Set2') \n plt.show() \n\n agglom.n_leaves_```",

"cod ho":"```lista_acc=[] \n for i in range(100): \n X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, stratify=y)  \n clf = neighbors.KNeighborsClassifier(13) \n clf.fit(X_train, y_train) \n yhat = clf.predict(X_test)\n #print('Accuracy: ', accuracy_score(y_test,yhat)) \n lista_acc.append(accuracy_score(y_test,yhat)) \n np.array(lista_acc).mean()```",
"cod loo":"```from sklearn import neighbors \n from sklearn.model_selection import LeaveOneOut \n X.shape, y.shape \n loo = LeaveOneOut() \n yhat=[] \n cont=0 \n for train_index, test_index in loo.split(X): \n #EL for va a realizar 150 iteraciones porq es un total de 150 patrones \n \t X_train, X_test = X[train_index], X[test_index] \n \t y_train, y_test = y[train_index], y[test_index] \n \t #CLASIFICADOR \n \t clf = neighbors.KNeighborsClassifier(3) \n \t clf.fit(X_train, y_train) \n \t yhat1 = clf.predict(X_test) \n \t yhat.append(yhat1) \n print('Accuracy: ', accuracy_score(y,yhat))```",
"cod kfl":"```from sklearn.model_selection import cross_val_score \n from sklearn.metrics import make_scorer \n clasificador = neighbors.KNeighborsClassifier(3) \n scores = cross_val_score(clasificador, X, y, cv=10, scoring = make_scorer(accuracy_score)) \n print(scores) \n print('*****'*10) \n print('Accuracy: ', scores.mean())```",
"cod skf":"```y2=list() \n lista_yhat=[] \n from sklearn.model_selection import StratifiedKFold \n skf = StratifiedKFold(n_splits=5) \n skf.get_n_splits(X, y)\n for train_index, test_index in skf.split(X, y): \n\t X_train, X_test = X[train_index], X[test_index]\n\t y_train, y_test = y[train_index], y[test_index] \n\t clasificador.fit(X_train, y_train)\n\t yhat = clasificador.predict(X_test) \n\t y2.extend(y_test)\n\t lista_yhat.extend(yhat)```",

"cod acs":"```from sklearn.metrics import jaccard_score \n from sklearn.metrics import accuracy_score \n from sklearn.metrics import precision_score \n from sklearn.metrics import recall_score \n from sklearn.metrics import f1_score \n from sklearn.metrics import confusion_matrix \n from sklearn.metrics import plot_confusion_matrix \n\n print('Jaccard index: ' , jaccard_score(y2, lista_yhat, average='macro')) \n print('Exactitud: ' , accuracy_score(y2, lista_yhat)) \n print('Precisión: ', precision_score(y2, lista_yhat, average='macro')) \n print('Sensibilidad: ', recall_score(y2, lista_yhat, average='macro')) \n print('F1-score: ', f1_score(y2, lista_yhat, average='macro')) \n\n from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n cm = confusion_matrix(y2, lista_yhat) \n print(cm) \n disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n disp.plot()```",
"cod pca":"```from sklearn.decomposition import PCA \n pca = PCA \n (n_components=3)\n X_train_pca = pca.fit_transform(X)``` " ,
"cod vpca":"```from sklearn.decomposition import PCA \n pca = PCA() \n X_train_pca = pca.fit_transform(X) \n pca.explained_variance_ratio_ \n plt.figure(figsize=(8,6)) \n plt.bar(range(1, len(numerodecolumnas)), pca.explained_variance_ratio_) \n plt.ylabel('Varianza explicada')   # puedo definir un corte en la cantidad de Z que quiero de acuerdo a la cantidad acumulada de varianza explicada. \n  plt.xlabel('PCA Index')  # lo ideal es quedarse con una varianza acumulada del 60% (en este caso las tres primeras barras explcian el 60% de la varianza explicada) \n plt.show() \n #pca.explained_variance_ratio_.cumsum()```" 




}]